{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-07 19:44:44.639554: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import datasets\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from torchvision.transforms import Compose, Normalize, ToTensor, Resize\n",
    "from transformers import AutoFeatureExtractor, AutoModelForImageClassification, TrainingArguments, Trainer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "checkpoint = \"microsoft/resnet-18\"\n",
    "folder_path = './images/'\n",
    "csv_path = './scp_codes.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(csv_path, delimiter=\";\")\n",
    "images, labels = [], []\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    image_name = row['filename_hr']\n",
    "    image_path = os.path.join(folder_path, f\"{image_name}.jpg\")\n",
    "    if os.path.exists(image_path):\n",
    "        images.append(Image.open(image_path))\n",
    "        labels.append(row['normal'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, hold_X, train_y, hold_y = train_test_split(images[:1000], labels[:1000], test_size=0.2)\n",
    "eval_X, test_X, eval_y, test_y = train_test_split(hold_X, hold_y, test_size=0.5)\n",
    "\n",
    "train_dataset = datasets.Dataset.from_dict({\"image\": train_X, \"label\": train_y})\n",
    "eval_dataset = datasets.Dataset.from_dict({\"image\": eval_X, \"label\": eval_y})\n",
    "test_dataset = datasets.Dataset.from_dict({\"image\": test_X, \"label\": test_y})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mosi/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages/transformers/models/convnext/feature_extraction_convnext.py:28: FutureWarning: The class ConvNextFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ConvNextImageProcessor instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "extractor = AutoFeatureExtractor.from_pretrained(checkpoint)\n",
    "normalize = Normalize(mean=extractor.image_mean, std=extractor.image_std)\n",
    "resize = Resize((extractor.size['shortest_edge'], extractor.size['shortest_edge']))\n",
    "class DivideBy255:\n",
    "    def __call__(self, image):\n",
    "        return image / 255.0\n",
    "\n",
    "transform = Compose([resize, ToTensor(), normalize])\n",
    "\n",
    "def preprocess(example):\n",
    "    example[\"pixel_values\"] = [transform(image.convert('RGB')) for image in example[\"image\"]]\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.set_transform(preprocess)\n",
    "eval_dataset.set_transform(preprocess)\n",
    "test_dataset.set_transform(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=2000x1200>,\n",
       " 'label': 1,\n",
       " 'pixel_values': tensor([[[1.3584, 1.6495, 1.6495,  ..., 1.6495, 1.6495, 1.6495],\n",
       "          [1.8893, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2147],\n",
       "          [1.9064, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2147],\n",
       "          ...,\n",
       "          [1.9064, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2147],\n",
       "          [1.9064, 2.2489, 2.2489,  ..., 2.2489, 2.2489, 2.2147],\n",
       "          [1.8893, 2.2147, 2.2147,  ..., 2.2147, 2.2147, 2.1975]],\n",
       " \n",
       "         [[1.5182, 1.8158, 1.8158,  ..., 1.8158, 1.8158, 1.8158],\n",
       "          [2.0609, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.3936],\n",
       "          [2.0784, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.3936],\n",
       "          ...,\n",
       "          [2.0784, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.3936],\n",
       "          [2.0784, 2.4286, 2.4286,  ..., 2.4286, 2.4286, 2.3936],\n",
       "          [2.0609, 2.3936, 2.3936,  ..., 2.3936, 2.3936, 2.3761]],\n",
       " \n",
       "         [[1.7337, 2.0300, 2.0300,  ..., 2.0300, 2.0300, 2.0300],\n",
       "          [2.2740, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6051],\n",
       "          [2.2914, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6051],\n",
       "          ...,\n",
       "          [2.2914, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6051],\n",
       "          [2.2914, 2.6400, 2.6400,  ..., 2.6400, 2.6400, 2.6051],\n",
       "          [2.2740, 2.6051, 2.6051,  ..., 2.6051, 2.6051, 2.5877]]])}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ResNetForImageClassification were not initialized from the model checkpoint at microsoft/resnet-18 and are newly initialized because the shapes did not match:\n",
      "- classifier.1.weight: found shape torch.Size([1000, 512]) in the checkpoint and torch.Size([2, 512]) in the model instantiated\n",
      "- classifier.1.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([2]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForImageClassification.from_pretrained(\n",
    "    checkpoint,\n",
    "    label2id = {'Normal': 1, 'Abnormal': 0},\n",
    "    id2label = {'1': 'Normal', '0': 'Abnormal'},\n",
    "    ignore_mismatched_sizes = True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0r/m7d3t2y16fqd36l8vj0jb5_c0000gn/T/ipykernel_2454/1255714581.py:22: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = datasets.load_metric(\"accuracy\")\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./output\",\n",
    "    remove_unused_columns=False,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    gradient_accumulation_steps=4,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    warmup_ratio=0.1,\n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\"\n",
    ")\n",
    "\n",
    "def collate_fn(examples):\n",
    "    pixel_values = torch.stack([example[\"pixel_values\"] for example in examples])\n",
    "    labels = torch.tensor([example[\"label\"] for example in examples])\n",
    "    return {\"pixel_values\": pixel_values, \"labels\": labels}\n",
    "\n",
    "metric = datasets.load_metric(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"Computes accuracy on a batch of predictions\"\"\"\n",
    "    predictions = np.argmax(eval_pred.predictions, axis=1)\n",
    "    return metric.compute(predictions=predictions, references=eval_pred.label_ids)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=extractor,\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=collate_fn,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mosi/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23cc07d00cf54066a5ef37cf38f99fdc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6445, 'learning_rate': 4.0625000000000005e-05, 'epoch': 0.8}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3511dedcc4b2496e808bdd732851683d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.48189041018486023, 'eval_accuracy': 0.75, 'eval_runtime': 8.2428, 'eval_samples_per_second': 12.132, 'eval_steps_per_second': 0.849, 'epoch': 0.96}\n",
      "{'loss': 0.4221, 'learning_rate': 2.5e-05, 'epoch': 1.6}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b72515c09db646dea650fd2d2d84c507",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4240489900112152, 'eval_accuracy': 0.79, 'eval_runtime': 8.2148, 'eval_samples_per_second': 12.173, 'eval_steps_per_second': 0.852, 'epoch': 2.0}\n",
      "{'loss': 0.3787, 'learning_rate': 9.375000000000001e-06, 'epoch': 2.4}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d172cb5ff694cfc8cf729b2a6e46bc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.41271865367889404, 'eval_accuracy': 0.78, 'eval_runtime': 8.1836, 'eval_samples_per_second': 12.22, 'eval_steps_per_second': 0.855, 'epoch': 2.88}\n",
      "{'train_runtime': 395.2942, 'train_samples_per_second': 6.071, 'train_steps_per_second': 0.091, 'train_loss': 0.45118772983551025, 'epoch': 2.88}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=36, training_loss=0.45118772983551025, metrics={'train_runtime': 395.2942, 'train_samples_per_second': 6.071, 'train_steps_per_second': 0.091, 'train_loss': 0.45118772983551025, 'epoch': 2.88})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16867a85d41149daaa3f429f9fbb7c53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** test metrics *****\n",
      "  epoch                   =       2.88\n",
      "  eval_accuracy           =        0.8\n",
      "  eval_loss               =     0.5051\n",
      "  eval_runtime            = 0:00:07.88\n",
      "  eval_samples_per_second =     12.689\n",
      "  eval_steps_per_second   =      0.888\n"
     ]
    }
   ],
   "source": [
    "metrics = trainer.evaluate(test_dataset)\n",
    "\n",
    "trainer.log_metrics(\"test\", metrics)\n",
    "trainer.save_metrics(\"test\", metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lewagon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
